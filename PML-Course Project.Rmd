---
title: "PML - Course Project"
date: "Wednesday, February 11, 2015"
output: html_document
---

```{r message=FALSE, warning=FALSE}
library(caret)
library(rattle)
library(randomForest)
library(rpart)
```

## Get and load data

First, we read the file and load raw data:
```{r get_data_file, CACHE = TRUE}
dataDirectory <- file.path(".","data")
myTrainingURL <- "https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv"
myTestingURL <- "https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv"
trainingFile <- file.path(dataDirectory, "pml-training.csv")
testingFile <- file.path(dataDirectory, "pml-testing.csv")

if(!file.exists(dataDirectory)) {
    dir.create(dataDirectory)
}

if (!file.exists(trainingFile)) {
    download.file(url = myTrainingURL, destfile = trainingFile, method = "auto")
}
if (!file.exists(testingFile)) {
    download.file(url = myTestingURL, destfile = testingFile, method = "auto")
}

#read the data, taking care with all strange NAs
rawDataTraining <- read.csv(trainingFile, na.strings=c("NA","#DIV/0!",""))
rawDataTesting <- read.csv(testingFile, na.strings=c("NA","#DIV/0!",""))
```


## Explore data

```{r results='hide'}
dim(rawDataTraining)
dim(rawDataTesting)
```

From http://groupware.les.inf.puc-rio.br/har

*In this work ([see the paper][2]) we first define quality of execution and investigate three aspects that pertain toqualitative activity recognition: the problem of specifying correct execution, the automatic and robust detection of execution mistakes, and how to provide feedback on the quality of execution to the user. We tried out an on-body sensing approach ([dataset here][3]), but also an "ambient sensing approach" (by using Microsoft Kinect - dataset still unavailable)*

*Six young health participants were asked to perform one set of 10 repetitions of the Unilateral Dumbbell Biceps Curl in five different fashions: exactly according to the specification (Class A), throwing the elbows to the front (Class B), lifting the dumbbell only halfway (Class C), lowering the dumbbell only halfway (Class D) and throwing the hips to the front (Class E).*

*Class A corresponds to the specified execution of the exercise, while the other 4 classes correspond to common mistakes. Participants were supervised by an experienced weight lifter to make sure the execution complied to the manner they were supposed to simulate. The exercises were performed by six male participants aged between 20-28 years, with little weight lifting experience. We made sure that all participants could easily simulate the mistakes in a safe and controlled manner by using a relatively light dumbbell (1.25kg).*

Read more: http://groupware.les.inf.puc-rio.br/har#ixzz3RRs6jF2n

[2]:http://groupware.les.inf.puc-rio.br/work.jsf?p1=11201
[3]:http://groupware.les.inf.puc-rio.br/static/WLE/WearableComputing_weight_lifting_exercises_biceps_curl_variations.csv

I'm gonna see in what columns there is a large proportion of NAs:
```{r}
# Return % of non-NAs in a data frame column
testNAs <- function(col_name, data){
    good <- !is.na(data[, col_name])
    return (sum(good)/length(data[, col_name]))
}

NAs_Testing <- as.data.frame(sapply(names(rawDataTesting), f <- function(x) {
    testNAs(x, rawDataTesting)
}))

NAs_Training <- as.data.frame(sapply(names(rawDataTraining), f <- function(x) {
    testNAs(x, rawDataTraining)
}))
```

## Clean data

I'll keep as predictor just those variables in testing data set with less than 50%  NAs.


```{r}
# Remove columns with more than 50 % NAs
tidyDataTesting <- rawDataTesting[, NAs_Training > 0.5]
tidyDataTraining <- rawDataTraining[, NAs_Training > 0.5]

dim(tidyDataTesting)  # Check
dim(tidyDataTraining)

tidyDataTesting <- tidyDataTesting[, -60] # Remove problem_id
tidyDataTraining$classe <- rawDataTraining$classe

# 1st 2 columns (id and name) have no info for our purposes
tidyDataTesting <- tidyDataTesting[c(-1,-2)]
tidyDataTraining <- tidyDataTraining[c(-1,-2)]

dim(tidyDataTesting) # Check
dim(tidyDataTraining)

# Some columns have different classes in tidyDataTraining and tidyDataTesting.
# Fix it.
for (name in names(tidyDataTesting)) {
    if (class(tidyDataTesting[, name]) != class(tidyDataTraining[, name]) )
        print(sprintf("%s - %s %s", name, class(tidyDataTesting[, name]), 
                                     class(tidyDataTraining[, name])))
    }

tidyDataTesting[, "magnet_dumbbell_z"] <- 
    as.numeric(tidyDataTesting[, "magnet_dumbbell_z"])
tidyDataTesting[, "magnet_forearm_y"] <- 
    as.numeric(tidyDataTesting[, "magnet_forearm_y"])
tidyDataTesting[, "magnet_forearm_z"] <- 
    as.numeric(tidyDataTesting[, "magnet_forearm_z"])

# Check it out.
identical(names(tidyDataTesting[,1:57]), names(testing[,1:57]))
identical(class(tidyDataTesting[,1:57]), class(testing[,1:57]))

# And to make sure it really worked, we'll use this simple technique:
tidyDataTesting <- rbind(tidyDataTraining[2, -58] , tidyDataTesting) 
tidyDataTesting <- tidyDataTesting[-1,]
```

## Prepare data sets for cross-validated training and testing

We'll follow this cross-validation startaegy: 

1. Create a `training` data set and a `testing` data set from our `tidyDataTraining` data set. 
2. Train models on the first, test them on the second 
3. And predict for `tidyDataTesting`.


```{r}
# Create training dataset and out of sample testing dataset
inTrain <- createDataPartition(y = tidyDataTraining$classe,
                              p=0.6, list=FALSE)
training <- tidyDataTraining[inTrain,]
testing <- tidyDataTraining[-inTrain,]
dim(training); dim(testing)
```

## Create Models and Predictions

We're gonna try several models and compare their performance later. In all the models:

- We train the model on `training`.
- Use the new model to predict on `testing`.
- Check the results with a Confussion Matrix and model statistics.

### 1. Decision Tree

```{r cache=TRUE}
modFit_DT <- rpart(classe ~ ., data= training, method="class")

predictions_DT <- predict(modFit_DT, testing, type = "class")

cm_DT <- confusionMatrix(predictions_DT, testing$classe)
cm_DT
```

Accuracy is `r round(100*cm_DT$overall[1],3)` %, quite good but sure we can beat it.

### 2. Random Forest
```{r cache=TRUE}

modFit_RF <- randomForest(classe ~., data = training)

predictions_RF <- predict(modFit_RF, testing, type = "class")

cm_RF <- confusionMatrix(predictions_RF, testing$classe)
cm_RF
```

Wow! Very good accuracy (`r round(100*cm_RF$overall[1],3)` %) indeed!

### 3. Boosting
```{r cache=TRUE, message=FALSE, warning=FALSE}
modFitBoosting <- train(classe ~ ., method = "gbm", data = training, 
                        verbose = FALSE )
print(modFitBoosting)

predictions_Boosting <- predict(modFitBoosting, testing)

cm_Boosting <- confusionMatrix(predictions_Boosting, testing$classe)
cm_Boosting
```


Boosting has become a hard competitor to Random Forest... (Accuracy = `r round(100*cm_Boosting$overall[1], 3)` %).

### 4. Naive Bayes
```{r cache=TRUE, message=FALSE, warning=FALSE}
modfit_nb <- train(factor(classe) ~ ., method = "nb", data = training)

predictions_nb <- predict(modfit_nb, testing)

cm_nb <-confusionMatrix(predictions_nb, testing$classe)
cm_nb 
```

What a deception! Just a `r round(100*cm_nb$overall[1], 3)` % accuracy...

### 5. Linear Discriminant Analysis
```{r cache=TRUE, message=FALSE, warning=FALSE}
modfit_lda <- train(factor(classe) ~ ., method = "lda", data = training)

predictions_lda <- predict(modfit_lda, testing)

cm_lda <- confusionMatrix(predictions_lda, testing$classe)
cm_lda

```

Not enough (Accuracy = `r round(100*cm_lda$overall[1],3)` %).

## Selecting a model

Here we have a table summing up the resulta of our 5 models:

| Model                        |  Accuracy lower            | Accuracy                  | Accuracy upper            | Out-of-Sample Error           | P-Value                    |
|:----------------------------:|:--------------------------:|:-------------------------:|:-------------------------:|:-----------------------------:|:---------------------------|
| Decision Tree                | `r cm_DT$overall[3]`       | `r cm_DT$overall[1]`      | `r cm_DT$overall[4]`      | `r 1 - cm_DT$overall[1]`      | `r cm_DT$overall[6]`       |
| Random Forest                | `r cm_RF$overall[3]`       | `r cm_RF$overall[1]`      | `r cm_RF$overall[4]`      | `r 1 - cm_RF$overall[1]`      | `r cm_RF$overall[6]`       |
| Boosting                     | `r cm_Boosting$overall[3]` | `r cm_Boosting$overall[1]`| `r cm_Boosting$overall[4]`| `r 1 - cm_Boosting$overall[1]`| `r cm_Boosting$overall[6]` |
| Linear Discriminant Analysis | `r cm_lda$overall[3]`      | `r cm_lda$overall[1]`     | `r cm_lda$overall[4]`     | `r 1 - cm_lda$overall[1]`     | `r cm_lda$overall[6]`      |
| Naive Bayes                  | `r cm_nb$overall[3]`       | `r cm_nb$overall[1]`      | `r cm_nb$overall[4]`      | `r 1 - cm_nb$overall[1]`      | `r cm_nb$overall[6]`       |

From the above, we select the Random Forest Model, in hard competiotn with Boosting.

## Prediction and files to submit
For Random Forest we use the following prediction on `tidyDataTesting`:

```{r}
predictions_RF_Testing <- as.character(predict(modFit_RF, tidyDataTesting))
```

And this is the code to generate files with predictions to submit for assignment:

```
pml_write_files = function(x, dir) {

  if(!file.exists(dir)) {
    dir.create(dir)
  }

  n = length(x)
  for(i in 1:n){
    filename = file.path(dir, paste0("problem_id_",i,".txt"))
    write.table(x[i], file=filename,quote=FALSE, 
    row.names=FALSE, col.names=FALSE, eol="")
  }
}

solDirectory <- file.path(".","solutions")

pml_write_files(predictions_RF_Testing, solDirectory)
```